{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "DataPreprocessing_0.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDlSWo_9pJlH",
    "outputId": "1b28f8e8-4de0-4ade-bada-3105a1af3426"
   },
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001B[32mOK\u001B[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nRMoh_rsLYU",
    "outputId": "e3fbe4d1-a2e6-4187-a4dd-c76e0185b8f2"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZA9qIyFYsnSk"
   },
   "source": [
    "!cp drive/MyDrive/masterThesis/scripts/preprocessing/* . -r"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lvLhte92xwJG"
   },
   "source": [
    "#!unzip drive/MyDrive/masterThesis/data/twitter/Twitter.parquet.zip -d drive/MyDrive/masterThesis/data/twitter/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "arv7h_ZAs1MR"
   },
   "source": [
    "!pip install pandarallel\n",
    "!pip install pyarrow\n",
    "!pip install transformers\n",
    "!pip install gensim==4.1.2\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "Ts9EM9FbrSkf"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=4, progress_bar= True)\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from utlities import strip_tweets\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from techniques import *\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "vCHy_doWrSkm"
   },
   "source": [
    "finbert = BertModel.from_pretrained('yiyanghkust/finbert-tone')\n",
    "finbert_class = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "pipe_bertEmbed = pipeline(\"feature-extraction\", model=finbert, tokenizer=tokenizer)\n",
    "pipe_bertSent = pipeline(\"sentiment-analysis\", model=finbert_class, tokenizer=tokenizer)\n",
    "\n",
    "input_path = \"drive/MyDrive/masterThesis/data/twitter/Twitter.parquet\"\n",
    "output_path = \"drive/MyDrive/masterThesis/data/twitter/Twitter.proc.parquet\"\n",
    "\n",
    "datatype = \"Twitter\"\n",
    "\n",
    "# only look at top 10 CCs\n",
    "ccs = ['btc', 'eth', 'xrp', 'xem', 'etc', 'ltc', 'dash', 'xmr', 'strat', 'xlm']\n",
    "\n",
    "counts = {}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "56zCdAmBrSkn"
   },
   "source": [
    "for cc in tqdm(ccs):\n",
    "    print(cc)\n",
    "    dataset = pq.ParquetDataset(input_path, validate_schema=False, filters=[('cc', '=', cc.upper())])\n",
    "    dataset = dataset.read().to_pandas()\n",
    "    full_shape = dataset.shape[0]\n",
    "\n",
    "    #apply filtering rules\n",
    "    if datatype == \"Twitter\":\n",
    "        filter_words = [\"give away\", \"giving away\", \"pump\", \"register\", \"join\", \"follow\"]\n",
    "        for filter_word in filter_words:\n",
    "            dataset = dataset.loc[~dataset[\"content\"].apply(lambda x: filter_word in x.lower())]\n",
    "\n",
    "        dataset = dataset.loc[dataset[\"content\"].apply(lambda x: x.count(\"#\") < 14)]\n",
    "        dataset = dataset.loc[dataset[\"content\"].apply(lambda x: x.count(\"$\") < 14)]\n",
    "        dataset = dataset.loc[dataset[\"content\"].apply(lambda x: x.count(\"@\") < 14)]\n",
    "        dataset = dataset.loc[dataset[\"content\"].apply(lambda x: x.count(\"|\") < 4)]\n",
    "\n",
    "        dataset = dataset.loc[dataset[\"content\"].apply(lambda x: len(strip_tweets(x)) > 20)]\n",
    "\n",
    "        # track how much data is lost due to filtering\n",
    "        counts[cc] = [full_shape, dataset.shape[0]]\n",
    "    else:\n",
    "        counts[cc] = [full_shape, full_shape]\n",
    "\n",
    "    #start preprocessing\n",
    "    dataset.rename(columns = {'content':'content_raw'}, inplace = True)\n",
    "\n",
    "    # for testing\n",
    "    dataset = dataset.iloc[:300, :]\n",
    "\n",
    "    # clear up text\n",
    "    print(\"preprocessing\")\n",
    "    dataset[\"content_processed\"] = dataset[\"content_raw\"].parallel_apply(lambda x: preprocessor(x))\n",
    "\n",
    "    # create w2v embeds\n",
    "    dataset[\"content_w2v\"] = dataset[\"content_processed\"].progress_apply(lambda x: generateW2V(x))\n",
    "    dataset[\"content_w2vSum\"] = dataset[\"content_w2v\"].progress_apply(lambda x: x.mean(axis=0).tolist())\n",
    "    dataset[\"content_w2v\"] = dataset[\"content_w2v\"].progress_apply(lambda x: x.tolist())\n",
    "\n",
    "    # create FinBERT embeds\n",
    "    dataset[\"content_bert\"] = pipe_bertEmbed(dataset[\"content_raw\"].to_list())\n",
    "    dataset[\"content_bert\"] = dataset[\"content_bert\"].progress_apply(lambda x: x.tolist())\n",
    "\n",
    "    # create Loughran McDonald sentiment scores\n",
    "    dataset[\"sentiment_LM\"] = dataset[\"content_processed\"].progress_apply(lambda x: generateLMSentimentScore(x))\n",
    "\n",
    "    # create Vader sentiment scores\n",
    "    dataset[\"sentiment_Vader\"] = dataset[\"content_processed\"].progress_apply(lambda x: generateVaderSentimentScore(x))\n",
    "\n",
    "    # create bert sentiment scores\n",
    "    dataset[\"sentiment_bert\"] = pipe_bertSent(dataset[\"content_raw\"].to_list())\n",
    "    dataset[\"sentiment_bert\"] = dataset[\"sentiment_bert\"].progress_apply(lambda x: BERTSentimentConversionDict[x[\"label\"]] * x[\"score\"])\n",
    "\n",
    "    # Write direct to parquet file\n",
    "    table = pa.Table.from_pandas(dataset)\n",
    "    pq.write_to_dataset(table, root_path=output_path, partition_cols=['date', 'source', 'cc'])"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}